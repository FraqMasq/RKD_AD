{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\my-torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_model_summary import summary\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "path = os.path.join(cwd, 'data')\n",
    "path = os.path.join(path, 'mvtec_AD')\n",
    "path = os.path.join(path, 'bottle')\n",
    "path = os.path.join(path, 'train')\n",
    "path = os.path.join(path, 'good')\n",
    "#filename = os.path.join(path, '000.png')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "img_h = 224\n",
    "img_w = 224\n",
    "# Wide-ResNet preprocessing\n",
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((img_h,img_w)),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_ids, image_dir, transform, target_transform=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(os.path.join(self.image_dir, self.image_ids[index]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        x, y = image, image\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset = MyDataset(os.listdir(path), path, preprocess)\n",
    "batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "168\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(train_loader.sampler))\n",
    "print(len(validation_loader.sampler))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CustomResNet, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Sequential(model.conv1,\n",
    "                                model.bn1,\n",
    "                                model.relu,\n",
    "                                model.maxpool,\n",
    "                                model.layer1)\n",
    "        self.l2 = nn.Sequential(\n",
    "                                model.layer2)\n",
    "        self.l3 = nn.Sequential(\n",
    "                                model.layer3)\n",
    "    def forward(self, x):\n",
    "        x1 = self.l1(x)\n",
    "        x2 = self.l2(x1)\n",
    "        x3 = self.l3(x2)\n",
    "        return x1, x2, x3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Type, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding.\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution.\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class OCBE(nn.Module):\n",
    "    \"\"\"One-Class Bottleneck Embedding module.\n",
    "    Args:\n",
    "        block (Bottleneck): Expansion value is extracted from this block.\n",
    "        layers (int): Numbers of OCE layers to create after multiscale feature fusion.\n",
    "        groups (int, optional): Number of blocked connections from input channels to output channels.\n",
    "            Defaults to 1.\n",
    "        width_per_group (int, optional): Number of layers in each intermediate convolution layer. Defaults to 64.\n",
    "        norm_layer (Optional[Callable[..., nn.Module]], optional): Batch norm layer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[Bottleneck, BasicBlock]],\n",
    "        layers: int,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.inplanes = 256 * block.expansion\n",
    "        self.dilation = 1\n",
    "        self.bn_layer = self._make_layer(block, 512, layers, stride=2)\n",
    "\n",
    "        self.conv1 = conv3x3(64 * block.expansion, 128 * block.expansion, 2)\n",
    "        self.bn1 = norm_layer(128 * block.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(128 * block.expansion, 256 * block.expansion, 2)\n",
    "        self.bn2 = norm_layer(256 * block.expansion)\n",
    "        self.conv3 = conv3x3(128 * block.expansion, 256 * block.expansion, 2)\n",
    "        self.bn3 = norm_layer(256 * block.expansion)\n",
    "\n",
    "        # This is present in the paper but not in the original code. With some initial experiments, removing this leads\n",
    "        # to better results\n",
    "        # self.conv4 = conv1x1(256 * block.expansion * 3, 256 * block.expansion * 3, 1)  # x3 as we concatenate 3 layers\n",
    "        # self.bn4 = norm_layer(256 * block.expansion * 3)\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[Bottleneck, BasicBlock]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes * 3, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes * 3,\n",
    "                planes,\n",
    "                stride,\n",
    "                downsample,\n",
    "                self.groups,\n",
    "                self.base_width,\n",
    "                previous_dilation,\n",
    "                norm_layer,\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, features: List[Tensor]) -> Tensor:\n",
    "        \"\"\"Forward-pass of Bottleneck layer.\n",
    "        Args:\n",
    "            features (List[Tensor]): List of features extracted from the encoder.\n",
    "        Returns:\n",
    "            Tensor: Output of the bottleneck layer\n",
    "        \"\"\"\n",
    "        # Always assumes that features has length of 3\n",
    "        feature0 = self.relu(self.bn2(self.conv2(self.relu(self.bn1(self.conv1(features[0]))))))\n",
    "        feature1 = self.relu(self.bn3(self.conv3(features[1])))\n",
    "        feature_cat = torch.cat([feature0, feature1, features[2]], 1)\n",
    "        output = self.bn_layer(feature_cat)\n",
    "        #output = self.bn_layer(self.bn4(self.conv4(feature_cat)))\n",
    "\n",
    "        return output.contiguous()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, Type, Union, Tuple\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torchvision.models.resnet import conv1x1, conv3x3\n",
    "\n",
    "class DecoderBasicBlock(nn.Module):\n",
    "    \"\"\"Basic block for decoder ResNet architecture.\n",
    "    Args:\n",
    "        inplanes (int): Number of input channels.\n",
    "        planes (int): Number of output channels.\n",
    "        stride (int, optional): Stride for convolution and de-convolution layers. Defaults to 1.\n",
    "        upsample (Optional[nn.Module], optional): Module used for upsampling output. Defaults to None.\n",
    "        groups (int, optional): Number of blocked connections from input channels to output channels.\n",
    "            Defaults to 1.\n",
    "        base_width (int, optional): Number of layers in each intermediate convolution layer. Defaults to 64.\n",
    "        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n",
    "        norm_layer (Optional[Callable[..., nn.Module]], optional): Batch norm layer to use.Defaults to None.\n",
    "    Raises:\n",
    "        ValueError: If groups are not equal to 1 and base width is not 64.\n",
    "        NotImplementedError: If dilation is greater than 1.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        upsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 2\n",
    "        if stride == 2:\n",
    "            self.conv1 = nn.ConvTranspose2d(\n",
    "                inplanes, planes, kernel_size=2, stride=stride, groups=groups, bias=False, dilation=dilation\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.upsample = upsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, batch: Tensor) -> Tensor:\n",
    "        \"\"\"Forward-pass of de-resnet block.\"\"\"\n",
    "        identity = batch\n",
    "\n",
    "        out = self.conv1(batch)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            identity = self.upsample(batch)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBottleneck(nn.Module):\n",
    "    \"\"\"Bottleneck for Decoder.\n",
    "    Args:\n",
    "        inplanes (int): Number of input channels.\n",
    "        planes (int): Number of output channels.\n",
    "        stride (int, optional): Stride for convolution and de-convolution layers. Defaults to 1.\n",
    "        upsample (Optional[nn.Module], optional): Module used for upsampling output. Defaults to None.\n",
    "        groups (int, optional): Number of blocked connections from input channels to output channels.\n",
    "            Defaults to 1.\n",
    "        base_width (int, optional): Number of layers in each intermediate convolution layer. Defaults to 64.\n",
    "        dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n",
    "        norm_layer (Optional[Callable[..., nn.Module]], optional): Batch norm layer to use.Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        upsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 2\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        if stride == 2:\n",
    "            self.conv2 = nn.ConvTranspose2d(\n",
    "                width, width, kernel_size=2, stride=stride, groups=groups, bias=False, dilation=dilation\n",
    "            )\n",
    "        else:\n",
    "            self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.upsample = upsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, batch: Tensor) -> Tensor:\n",
    "        \"\"\"Forward-pass of de-resnet bottleneck block.\"\"\"\n",
    "        identity = batch\n",
    "\n",
    "        out = self.conv1(batch)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            identity = self.upsample(batch)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet model for decoder.\n",
    "    Args:\n",
    "        block (Type[Union[DecoderBasicBlock, DecoderBottleneck]]): Type of block to use in a layer.\n",
    "        layers (List[int]): List to specify number for blocks per layer.\n",
    "        zero_init_residual (bool, optional): If true, initializes the last batch norm in each layer to zero.\n",
    "            Defaults to False.\n",
    "        groups (int, optional): Number of blocked connections per layer from input channels to output channels.\n",
    "            Defaults to 1.\n",
    "        width_per_group (int, optional): Number of layers in each intermediate convolution layer.. Defaults to 64.\n",
    "        norm_layer (Optional[Callable[..., nn.Module]], optional): Batch norm layer to use. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[DecoderBasicBlock, DecoderBottleneck]],\n",
    "        layers: List[int],\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 512 * block.expansion\n",
    "        self.dilation = 1\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, DecoderBottleneck):\n",
    "                    nn.init.constant_(module.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(module, DecoderBasicBlock):\n",
    "                    nn.init.constant_(module.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[DecoderBasicBlock, DecoderBottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        upsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            upsample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    kernel_size=2,\n",
    "                    stride=stride,\n",
    "                    groups=self.groups,\n",
    "                    bias=False,\n",
    "                    dilation=self.dilation,\n",
    "                ),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, upsample, self.groups, self.base_width, previous_dilation, norm_layer)\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, batch: Tensor) -> List[Tensor]:\n",
    "        \"\"\"Forward pass for Decoder ResNet. Returns list of features.\"\"\"\n",
    "        feature_a = self.layer1(batch)  # 512*8*8->256*16*16\n",
    "        feature_b = self.layer2(feature_a)  # 256*16*16->128*32*32\n",
    "        feature_c = self.layer3(feature_b)  # 128*32*32->64*64*64\n",
    "\n",
    "        return [feature_c, feature_b, feature_a]\n",
    "\n",
    "def _resnet(block: Type[Union[DecoderBasicBlock, DecoderBottleneck]], layers: List[int], **kwargs: Any) -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model\n",
    "\n",
    "def de_wide_resnet50_2() -> ResNet:\n",
    "    \"\"\"Wide ResNet-50-2 model.\"\"\"\n",
    "    return _resnet(DecoderBottleneck, [3, 4, 6, 3], width_per_group=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Conv2d-1     [1, 64, 112, 112]           9,408               0\n",
      "     BatchNorm2d-2     [1, 64, 112, 112]             128               0\n",
      "            ReLU-3     [1, 64, 112, 112]               0               0\n",
      "       MaxPool2d-4       [1, 64, 56, 56]               0               0\n",
      "      Bottleneck-5      [1, 256, 56, 56]         206,336               0\n",
      "      Bottleneck-6      [1, 256, 56, 56]         214,016               0\n",
      "      Bottleneck-7      [1, 256, 56, 56]         214,016               0\n",
      "      Bottleneck-8      [1, 512, 28, 28]         920,576               0\n",
      "      Bottleneck-9      [1, 512, 28, 28]         854,016               0\n",
      "     Bottleneck-10      [1, 512, 28, 28]         854,016               0\n",
      "     Bottleneck-11      [1, 512, 28, 28]         854,016               0\n",
      "     Bottleneck-12     [1, 1024, 14, 14]       3,676,160               0\n",
      "     Bottleneck-13     [1, 1024, 14, 14]       3,411,968               0\n",
      "     Bottleneck-14     [1, 1024, 14, 14]       3,411,968               0\n",
      "     Bottleneck-15     [1, 1024, 14, 14]       3,411,968               0\n",
      "     Bottleneck-16     [1, 1024, 14, 14]       3,411,968               0\n",
      "     Bottleneck-17     [1, 1024, 14, 14]       3,411,968               0\n",
      "=========================================================================\n",
      "Total params: 24,862,528\n",
      "Trainable params: 0\n",
      "Non-trainable params: 24,862,528\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Testing components generation:\n",
    "from pytorch_model_summary import summary\n",
    "# load WRN-50-2:\n",
    "full_w_resnet = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)\n",
    "#encoder-architecture\n",
    "enc = CustomResNet(model=full_w_resnet)\n",
    "for param in enc.parameters():\n",
    "    param.requires_grad = False\n",
    "print(summary(enc, torch.zeros((1, 3, img_h, img_w))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Conv2d-1      [1, 512, 28, 28]       1,179,648       1,179,648\n",
      "     BatchNorm2d-2      [1, 512, 28, 28]           1,024           1,024\n",
      "            ReLU-3      [1, 512, 28, 28]               0               0\n",
      "          Conv2d-4     [1, 1024, 14, 14]       4,718,592       4,718,592\n",
      "     BatchNorm2d-5     [1, 1024, 14, 14]           2,048           2,048\n",
      "            ReLU-6     [1, 1024, 14, 14]               0               0\n",
      "          Conv2d-7     [1, 1024, 14, 14]       4,718,592       4,718,592\n",
      "     BatchNorm2d-8     [1, 1024, 14, 14]           2,048           2,048\n",
      "            ReLU-9     [1, 1024, 14, 14]               0               0\n",
      "     Bottleneck-10       [1, 2048, 7, 7]      11,282,432      11,282,432\n",
      "     Bottleneck-11       [1, 2048, 7, 7]       4,462,592       4,462,592\n",
      "     Bottleneck-12       [1, 2048, 7, 7]       4,462,592       4,462,592\n",
      "=========================================================================\n",
      "Total params: 30,829,568\n",
      "Trainable params: 30,829,568\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ocbe = OCBE(Bottleneck, 3)\n",
    "print(summary(ocbe, enc(torch.zeros((1, 3, img_h, img_w)))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "           Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "==============================================================================\n",
      "    DecoderBottleneck-1     [1, 1024, 14, 14]      11,016,192      11,016,192\n",
      "    DecoderBottleneck-2     [1, 1024, 14, 14]       3,411,968       3,411,968\n",
      "    DecoderBottleneck-3     [1, 1024, 14, 14]       3,411,968       3,411,968\n",
      "    DecoderBottleneck-4      [1, 512, 28, 28]       2,755,584       2,755,584\n",
      "    DecoderBottleneck-5      [1, 512, 28, 28]         854,016         854,016\n",
      "    DecoderBottleneck-6      [1, 512, 28, 28]         854,016         854,016\n",
      "    DecoderBottleneck-7      [1, 512, 28, 28]         854,016         854,016\n",
      "    DecoderBottleneck-8      [1, 256, 56, 56]         689,664         689,664\n",
      "    DecoderBottleneck-9      [1, 256, 56, 56]         214,016         214,016\n",
      "   DecoderBottleneck-10      [1, 256, 56, 56]         214,016         214,016\n",
      "   DecoderBottleneck-11      [1, 256, 56, 56]         214,016         214,016\n",
      "   DecoderBottleneck-12      [1, 256, 56, 56]         214,016         214,016\n",
      "   DecoderBottleneck-13      [1, 256, 56, 56]         214,016         214,016\n",
      "==============================================================================\n",
      "Total params: 24,917,504\n",
      "Trainable params: 24,917,504\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "decoder = de_wide_resnet50_2()\n",
    "print(summary(decoder, ocbe(enc(torch.zeros((1, 3, img_h, img_w))))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from kornia.filters import gaussian_blur2d\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class AnomalyMapGenerator:\n",
    "    \"\"\"Generate Anomaly Heatmap.\n",
    "    Args:\n",
    "        image_size (Union[ListConfig, Tuple]): Size of original image used for upscaling the anomaly map.\n",
    "        sigma (int): Standard deviation of the gaussian kernel used to smooth anomaly map.\n",
    "        mode (str, optional): Operation used to generate anomaly map. Options are `add` and `multiply`.\n",
    "                Defaults to \"multiply\".\n",
    "    Raises:\n",
    "        ValueError: In case modes other than multiply and add are passed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size: Tuple, sigma: int = 4, mode: str = \"multiply\"):\n",
    "        self.image_size = image_size\n",
    "        self.sigma = sigma\n",
    "        self.kernel_size = 2 * int(4.0 * sigma + 0.5) + 1\n",
    "\n",
    "        if mode not in (\"add\", \"multiply\"):\n",
    "            raise ValueError(f\"Found mode {mode}. Only multiply and add are supported.\")\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, student_features: List[Tensor], teacher_features: List[Tensor]) -> Tensor:\n",
    "        \"\"\"Computes anomaly map given encoder and decoder features.\n",
    "        Args:\n",
    "            student_features (List[Tensor]): List of encoder features\n",
    "            teacher_features (List[Tensor]): List of decoder features\n",
    "        Returns:\n",
    "            Tensor: Anomaly maps of length batch.\n",
    "        \"\"\"\n",
    "        if self.mode == \"multiply\":\n",
    "            anomaly_map = torch.ones(\n",
    "                [student_features[0].shape[0], 1, *self.image_size], device=student_features[0].device\n",
    "            )  # b c h w\n",
    "        elif self.mode == \"add\":\n",
    "            anomaly_map = torch.zeros(\n",
    "                [student_features[0].shape[0], 1, *self.image_size], device=student_features[0].device\n",
    "            )\n",
    "\n",
    "        for student_feature, teacher_feature in zip(student_features, teacher_features):\n",
    "            distance_map = 1 - F.cosine_similarity(student_feature, teacher_feature)\n",
    "            distance_map = torch.unsqueeze(distance_map, dim=1)\n",
    "            distance_map = F.interpolate(distance_map, size=self.image_size, mode=\"bilinear\", align_corners=True)\n",
    "            if self.mode == \"multiply\":\n",
    "                anomaly_map *= distance_map\n",
    "            elif self.mode == \"add\":\n",
    "                anomaly_map += distance_map\n",
    "\n",
    "        anomaly_map = gaussian_blur2d(\n",
    "            anomaly_map, kernel_size=(self.kernel_size, self.kernel_size), sigma=(self.sigma, self.sigma)\n",
    "        )\n",
    "\n",
    "        return anomaly_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "class ReverseDistillationModel(nn.Module):\n",
    "    \"\"\"Reverse Distillation Model.\n",
    "    Args:\n",
    "        backbone (str): Name of the backbone used for encoder and decoder\n",
    "        input_size (Tuple[int, int]): Size of input image\n",
    "        layers (List[str]): Name of layers from which the features are extracted.\n",
    "        anomaly_map_mode (str): Mode used to generate anomaly map. Options are between ``multiply`` and ``add``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, oceb, decoder, input_size = (224, 224), anomaly_map_mode: str = 'multiply'):\n",
    "        super().__init__()\n",
    "\n",
    "        for param in encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.encoder = encoder\n",
    "        self.oceb = oceb\n",
    "        self.decoder = decoder\n",
    "\n",
    "        image_size = input_size\n",
    "\n",
    "        self.anomaly_map_generator = AnomalyMapGenerator(image_size=tuple(image_size), mode=anomaly_map_mode)\n",
    "\n",
    "    def forward(self, images: Tensor) -> Union[Tensor, Tuple[List[Tensor], List[Tensor]]]:\n",
    "        \"\"\"Forward-pass images to the network.\n",
    "        During the training mode the model extracts features from encoder and decoder networks.\n",
    "        During evaluation mode, it returns the predicted anomaly map.\n",
    "        Args:\n",
    "            images (Tensor): Batch of images\n",
    "        Returns:\n",
    "            Union[Tensor, Tuple[List[Tensor],List[Tensor]]]: Encoder and decoder features in training mode,\n",
    "                else anomaly maps.\n",
    "        \"\"\"\n",
    "        self.encoder.eval()\n",
    "\n",
    "        encoder_features = self.encoder(images)\n",
    "        #encoder_features = list(encoder_features.values())\n",
    "\n",
    "        decoder_features = self.decoder(self.oceb(encoder_features))\n",
    "\n",
    "        if self.training:\n",
    "            output = encoder_features, decoder_features\n",
    "        else:\n",
    "            output = self.anomaly_map_generator(encoder_features, decoder_features)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                                              Output Shape         Param #     Tr. Param #\n",
      "=============================================================================================================\n",
      "    CustomResNet-1     [1, 256, 56, 56], [1, 512, 28, 28], [1, 1024, 14, 14]      24,862,528               0\n",
      "            OCBE-2                                           [1, 2048, 7, 7]      30,829,568      30,829,568\n",
      "          ResNet-3     [1, 256, 56, 56], [1, 512, 28, 28], [1, 1024, 14, 14]      24,917,504      24,917,504\n",
      "=============================================================================================================\n",
      "Total params: 80,609,600\n",
      "Trainable params: 55,747,072\n",
      "Non-trainable params: 24,862,528\n",
      "-------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "full_w_resnet = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)\n",
    "torch.cuda.empty_cache()\n",
    "model = ReverseDistillationModel(encoder=CustomResNet(model=full_w_resnet),\n",
    "                                 oceb=OCBE(Bottleneck, 3),\n",
    "                                 decoder=de_wide_resnet50_2()).to(device)\n",
    "\n",
    "print(summary(model, torch.zeros((1, 3, img_h, img_w)).to(device)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hiddenlayer as hl\n",
    "\n",
    "hl_graph = hl.build_graph(model, torch.zeros((1, 3, img_h, img_w)).to('cuda'))\n",
    "hl_graph.theme = hl.graph.THEMES[\"blue\"].copy()\n",
    "#hl_graph.save(path=\"model\" , format=\"jpg\")\n",
    "hl_graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "def kd_loss(encoder_features: List[Tensor], decoder_features: List[Tensor]) -> Tensor:\n",
    "    \"\"\"Computes cosine similarity loss based on features from encoder and decoder.\n",
    "    Args:\n",
    "    encoder_features (List[Tensor]): List of features extracted from encoder\n",
    "    decoder_features (List[Tensor]): List of features extracted from decoder\n",
    "    Returns:\n",
    "    Tensor: Cosine similarity loss\n",
    "    \"\"\"\n",
    "    cos_loss = torch.nn.CosineSimilarity()\n",
    "    losses = list(map(cos_loss, encoder_features, decoder_features))\n",
    "    loss_sum = 0\n",
    "    for loss in losses:\n",
    "        loss_sum += torch.mean(1 - loss)  # mean of cosine distance\n",
    "    return loss_sum\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 3         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    5620 MB |    5757 MB |     930 GB |     925 GB |\n",
      "|       from large pool |    5570 MB |    5707 MB |     928 GB |     922 GB |\n",
      "|       from small pool |      50 MB |      68 MB |       2 GB |       2 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    5620 MB |    5757 MB |     930 GB |     925 GB |\n",
      "|       from large pool |    5570 MB |    5707 MB |     928 GB |     922 GB |\n",
      "|       from small pool |      50 MB |      68 MB |       2 GB |       2 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6022 MB |    6022 MB |    6232 MB |  215040 KB |\n",
      "|       from large pool |    5968 MB |    5968 MB |    6148 MB |  184320 KB |\n",
      "|       from small pool |      54 MB |      72 MB |      84 MB |   30720 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  247513 KB |    1198 MB |  478090 MB |  477848 MB |\n",
      "|       from large pool |  243552 KB |    1189 MB |  475432 MB |  475194 MB |\n",
      "|       from small pool |    3961 KB |       9 MB |    2657 MB |    2653 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1446    |    1465    |  210097    |  208651    |\n",
      "|       from large pool |     357    |     361    |   45392    |   45035    |\n",
      "|       from small pool |    1089    |    1249    |  164705    |  163616    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1446    |    1465    |  210097    |  208651    |\n",
      "|       from large pool |     357    |     361    |   45392    |   45035    |\n",
      "|       from small pool |    1089    |    1249    |  164705    |  163616    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     196    |     205    |     214    |      18    |\n",
      "|       from large pool |     169    |     169    |     172    |       3    |\n",
      "|       from small pool |      27    |      36    |      42    |      15    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      97    |     144    |   90949    |   90852    |\n",
      "|       from large pool |      92    |     116    |   28761    |   28669    |\n",
      "|       from small pool |       5    |      42    |   62188    |   62183    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch num: 198: 100%|██████████| 199/199 [18:16<00:00,  5.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "# Model Initialization\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Using an Adam Optimizer with lr = 0.01\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr = 0.005,\n",
    "                             betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "outputs = []\n",
    "losses = []\n",
    "\n",
    "t = trange(range(epochs)[-1], desc=\"Epoch num %i\")\n",
    "\n",
    "#for epoch in range(epochs):\n",
    "for epoch in t:\n",
    "    t.set_description(\"Epoch num: %i\" % epoch)\n",
    "    t.refresh() # to show immediately the update\n",
    "    sleep(0.01)\n",
    "    for (image, _) in train_loader:\n",
    "        image =  image.to(device)\n",
    "        # Output of Teacher\n",
    "        encoded = model.encoder(image)\n",
    "        #Output of Student\n",
    "        reconstructed = model.decoder(model.oceb(encoded))\n",
    "        # Calculating the loss function\n",
    "        loss = kd_loss(encoded, reconstructed)\n",
    "\n",
    "        # The gradients are set to zero,\n",
    "        # the the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #outputs.append((epochs, image, reconstructed))\n",
    "        torch.cuda.empty_cache()\n",
    "    # Storing the losses in a list for plotting\n",
    "    losses.append(loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x1b2343217c0>]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cpu_losses = []\n",
    "for loss in losses:\n",
    "    cpu_losses.append(loss.to('cpu').detach().numpy())\n",
    "# Defining the Plot Style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "# Plotting the last 100 values\n",
    "plt.plot(cpu_losses)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import datetime\n",
    "item = \"bottle\"\n",
    "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_path =  os.path.join(cwd, \"SavedModels\")\n",
    "save_path =  os.path.join(save_path, '{1}_SavedModel{0}.pt'.format(time, item))\n",
    "torch.save(model.state_dict(),save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "ReverseDistillationModel(\n  (encoder): CustomResNet(\n    (l1): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (l2): Sequential(\n      (0): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (l3): Sequential(\n      (0): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (oceb): OCBE(\n    (bn_layer): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(3072, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (decoder): ResNet(\n    (layer1): Sequential(\n      (0): DecoderBottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (upsample): Sequential(\n          (0): ConvTranspose2d(2048, 1024, kernel_size=(2, 2), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): DecoderBottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): DecoderBottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): DecoderBottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (upsample): Sequential(\n          (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): DecoderBottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): DecoderBottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): DecoderBottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): DecoderBottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (upsample): Sequential(\n          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): DecoderBottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): DecoderBottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): DecoderBottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): DecoderBottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): DecoderBottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ReverseDistillationModel(encoder=CustomResNet(model=full_w_resnet),\n",
    "                                 oceb=OCBE(Bottleneck, 3),\n",
    "                                 decoder=de_wide_resnet50_2())\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "my-torch",
   "language": "python",
   "display_name": "my-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}